---
title: 'Agile Teams Quick Playbook'
date: '2023-10-30'
lastmod: '2023-10-30'
tags: ['agile', 'mvp', 'playbook']
draft: false
summary: 'Engineering quality into your SDLC by adopting a shift left team mindset can lead to higher quality outcomes with fewer bugs and happier customers'
images: ['/static/images/agilemvp.png']
---

# Introduction

We've all heard of Agile software development, but what does it really mean? How do you know if you're doing it right? How do you know if you're doing it wrong? How do you know if you're doing it at all?

By the end of this article, you should have a better understanding of what an Agile team looks like and some guidance on some best practices.

If you haven't already read these, give them a read:

- [Agile Manifesto](https://agilemanifesto.org/)
- [12 Principles](https://agilemanifesto.org/principles.html)
- [Modern Agile](https://modernagile.org/)

Let's get started...

# MVP - Minimum Viable Product:

There are hundreds of similar, but different definitions of MVP. Here is one more to add to the pile:

> The smallest deliverable that solves a problem or adds value

That is a pretty broad definition, but part of agile is leaving things open to interpretation and allowing teams to self-organize to meet the needs of the customer and themselves.

Building on that definition, we want to make sure we are _incrementally_ building on top of whatever we're shipping. Why?

Agile software development relies on continuous and incremental improvement, gathering feedback along the way. We do this so we can ensure we're calibrating our direction as much as possible.

You've probably seen this image below. But if you haven't, it illustrates that you can incrementally build towards an end goal (a Car in this case) while slowly improving the user's experience by adding value along the way.
If you need to get from Point A to Point B, you can technically use a skateboard to get there. A scooter will also work, but the scooter will be a bit more efficient than the skateboard. A bike will also be a bit better than a scooter, and so on.
The thing is, you don't always want a Car or a Motorcycle. If you only need to move 20 feet, why use a car?

Context matters for the problem you're trying to solve and you don't always want 'the fanciest solution'.
Those cars, while powerful in transportation, take a long time to build. So if you're going to build a car, you should either be really sure your users need a car or build some incremental steps along the way to validate the direction and end goal. You'll end up delighting your customers along the way by shipping those incremental bits of value.

![Agile MVP](/static/images/agilemvp.png)

Another core tenet of agile is being able to abandon ship or delete features at any time. Not everything we build will solve the problem we attempted to tackle. Sometimes things just don't stick.
That's OK! These are good outcomes as long as we were intentional in our process to arrive at the end state. And if we practiced good, incremental software delivery we know that we spent just enough time (and not more!) to come to a conculsion supported by real customer data.

Building software incrementally can also increase the quality of your product. By doing small bits of feature delivery, the total change set is also small. This allows for higher quality code reviews and more focused testing and risk analysis. If we spend 4 weeks building 1 feature and deliver it in a big-bang release, there's a high chance something was missed and there ends up being bugs out in the wild.

If you find yourself stuck, features are taking too long to ship, or a high degree of bugs are being introduced, use the following prompt in your next grooming or planning session to help refocus your team:

> What is the smallest bit of product we can deliver that adds value

# Grooming and Planning

This phase is the one which requires the most effort to 'get it right', but will yield the highest reward if successful. If this phase is shortcut or unattended to, be ready for a world of hurt.

Planning and Grooming are the steps that grease the gears for the rest of the SDLC. If you don't have clear, well defined pieces of work you leave yourself exposed to scope creep, missed requirements, and bugs.

When teams have an understanding of the risks of the domain they're working in and the business need is clear and well defined, engineers have the best chance to develop bug free software that is secure, scalable, reliable, and well tested.

If you're struggling to write good user stories, cards, tickets, or whatever you call them, here's some prompts and guidelines for what a good story could contain:

- Generally, the ticket should be understandable by anyone in your ogananization or business line. This means eliminating as much technical jargon from the core of the card as possible (save those for developer notes). You should have clear and concise descriptions. Acronyms are well understood otherwise avoided.
- Each ticket should have a user story. This is the building block for the rest of the ticket: "As a [user type], I want to [do something] so that [I can achieve some goal]." If you can't write a user story, you don't have a clear understanding of the problem you're trying to solve.
- If you're shipping features with UI components, attach Mock-ups or screenshots of what you're trying to build. This will help eliminate any visual design assumptions. If there aren't any mockups, make sure there is at least an acceptance critera or requirement to "Pair with UX" during the development process. Pairing with UX in the moment of development is a great way to iterate quickly on design and layout, rather than discovering the UI is faulty later on during the software delivery cycle.
- Attach relevant data warehouse or analytics queries
- Attach software documentation (3rd party or Internal), or API Docs
- Attach customer feedback, support tickets, or recorded customer calls so the team can fill in any gaps or resolve ambiguities
- Remove or qualify as many assumptions as possible. Assumptions can lead to divergence from the goal and can introduce bugs or missed requirements.
- Identify potential risks (internal and external) to product, team, customer, code, 3rd parties, etc
- Identify testing needs. Can the other developers test the ticket themselves? Does it need a specific Quality Engineer to test the ticket? More on this in [Quality Evolution](https://qe-ruby.vercel.app/blog/quality-evolution)
- Identify any code or inter/intra app dependencies. Does it need to be forward/backward compatible? What are the release and/or rollback stragegies if any?
- Idenitfy post release monitoring, metrics, KPIs, alerting, experiments, etc.

Remember to keep stories small and incremental. A little bit of delivered value today can make a big difference.

# Acceptance criteria

From all of the preparation work done in Planning and the beginning of Grooming, Acceptance Criteria (ACs, or requirements, etc) distill the above information down into specific units of work. These points should be concise, clear, and tie back to the business value. Where possible, each of these ACs should correspond to both:

- a code change
- an automated test

Even if your team doesn't practice TDD (Test Driven Development), you should still be writing automated tests to cover your code. Pairing app code with automation code, and treating automation code as a first class priority, will help your team move quickly and safely. Automation code is insurance to prevent future pain.

Planning and Grooming sessions can be long and tedious, but they are arguably the most important part of the SDLC. Because there is so much information to cover, it is best to follow this rule of thumb:

> If it's obvious, write it down

People won't remember what the team verbally agreed to a few days prior to picking up a ticket. The agreements are usually 'obvious' and 'common sense' at the time, but there is a laundry list of things that people are keeping track of in their head. Include the obvious units of work to be done as ACs so they aren't overlooked or forgotten. A note is easily buried in a description of a ticket. An AC is a specific call out, and very easy to check against.

Some tips for good Acceptance Criteria:

- NOT a “QA Checklist” (more on this in following articles)

- All should be checked off before code review

- Generally all should be manually tested by the developer prior to code review

- Generally all should have corresponding automated test(s)

- Concise yet descriptive

# Code Review, Testing & Release phase

In my experience, Code Review and final Testing phases are moments where the SDLC can come grinding to a halt if not managed effectively. Even within teams that practice Agile, the most 'waterfall-y' behaviors happen here.
So you may ask, how do we smooth these phases out and keep the momentum we've built in previous steps?

These steps take intentional driving through to release or merge. Adding WIP Limits (Work In Progress) has worked well to prevent things piling up in Code Review or In Final Testing.
Ensure your team has a well defined process for when something is finished with Code Review and ready for the next step. Does it need 1 approval, or multiple approvals? Do you communicate in a shared team channel when code review is complete? Do you have automated notifications set up to help ease the process?
Make sure you and your team are checking in daily during the standup ritual and address each column. This is a great opportunity to try a new stlye of Standup by 'walking the board'.
Walking the board is a method by which move from one status to the next all the way through to the beginning or end of your status'.

Let's take an example of a team that has a Kanban board structured with 8 columns:

- Backlog

- Ready for Grooming

- To Discuss

- Groomed/Ready to Pick up

- In Development

- In Code Review

- In Final Testing

- Merged/Released

Walking the board in this example you can start either with To Discuss and work your way "Left to Right" through to the Merged/Released column, or move from "Right to Left" by starting with the Merged/Released column and working your way back to To Discuss.
This is a helpful way to identify items that could be stuck, need attention, or reviewing what the team has released. It offers up moments for team members to talk about the traditional Status Updates normally discussed in standups like 'What I worked on Yesterday', 'What I will work on Today', and 'What I am blocked/need help on'

The reason I prefer Walking the Board vs Status Updates style, is Walking the Board gives space for work that falls outside of the status updates and keeps the team in sync on the progress on various items. Status updates, in my opinion, feel a bit micro-managery. Managing work in progress and keeping the flow feels like a more high value activity to me.

If you know the story/ticket will need final testing, you don't have to wait for the card to be done with Code Review to begin. Each step in the SDLC has space for performing testing. Whether it is

- Testing assumptions during Planning/Grooming

- Testing your new code during development

- Testing your peer's code during Code Review

- Exploratory Testing in the In Final Testing phase

- Testing expected outcomes post release

Final Testing shouldn't be the first time a story is being tested, this phase should be additional testing gestures to supplement and extend previous testing/quality assurance motions. When transitioning from Code Review to Final Testing, it helps to have already established who will be taking on the final testing or communicate to teammates when someone decides to own the testing on a story.

During Final Testing, you’re not re-verifying Acceptance criteria here. You’re building on top of and extending those! Why? This is because the ACs should already be covered by automated tests and verified prior to code review. This is where exploratory testing provides it's value. Checking off ACs on a ticket is NOT exploratory testing, and many people think that's what they're doing when it's not. Think creatively about the interactions between your new code and existing, and how that feature could impact other features or workflows. Go beyond the scope of the change and test the system as a whole.

Post release, circle back onto the release monitoring and metrics you established in grooming. Ideally you are leveraging tools like Sentry or Rollbar for exception reporting, DataDog or NewRelic for application metrics and custom event tracking, Snowflake or a Data warehouse to gather metrics, FullStory or Pendo or LogRocket for Click Tracking and Video replays. These tools help you inform the observation into your application and usage and let you know if you're on the right track, there's a bug, or you need to revisit an implementation.

# Meeting hygiene

We've all head the phrase 'I have too many meetings' or 'I don't have enough time'. Meetings are valuable when they're being used correctly but can be distracting and detrimental when not. I'd venture to suggest that most meetings aren't being run effectively and 'could have been an email'.
If you have 3 meetings a day, each day, and it [takes approximately 23 minutes](https://lifehacker.com/how-long-it-takes-to-get-back-on-track-after-a-distract-1720708353) to return to productivity on a task post-meeting, you could be wasting about 70 mins a day, or about 6 hours a week just trying to get back into your flow/focus.
You can see how important it is to not only make sure you've got good meeting hygiene, but also make scheduling a meeting an intentional act.

If you've got a lot of meetings, are all of them neccesary? Are you and your team just doing them because that's how it's always been done? Are those meetings still providing the same value as when they were originally created?
I bet the answer to at least one of those questions is 'No'.

If the meeting you have is indeed valuable, now move on to review how it is being run, how focused, and how 'true to the mission' the meeting is. It's extremely easy to succumb to 'scope creep' in meetings, discussing items that are not actual blockers to implementation/development. Also review the duration of the meeting. If you schedule an hour, usually your team will take the whole hour. Could it be 30 or 45 minutes instead? Scheduling a meeting for an hour could be a compensation for a lack of focus eating up large amounts of meeting time. Get the meeting agenda and discussions tightened up, and try fitting into smaller time windows so you can spend more time on delivery.

If possible, assign someone to be 'the focus keeper' or 'time keeper'. This role, while still participating in the meeting, is allowed to interrupt when they see the conversation taking a different direction or see something unproductive. An easy interupt is simply asking 'Is what we're discussing still relevant to the origianl topic', or 'Do we want to keep discussing this for 5 more minutes, or move on?'. It takes practice to recognize these moments, but having a psychologically safe and supported way to help keep teams focused will help everyone learn these skills and improve the team as a whole.

# Retrospectives

Taking time to reflect is an important part of the software lifecycle. This is a moment to celebrate wins and achievements, discuss process, identify areas for improvement, and get a dialogue going. Retro provides an avenue for continuous feedback on team health and process, constantly adjusting during the lifecycle of the team.

Celebration is important! The team is working hard towards goals, and pausing to congratulate one another and reflect on a job well done is imperitive for keeping morale high and momentum going. Use this opportunity to send your appreciates to your teammates and call out particularly exemplary performance. Share the love!

Retro also affords us the chance to discuss how things are going, how our process is inhibiting or accelerating us. We can reflect on what went well and what we want to change or try out for the next cycle. Nothing is set in stone, and taking a moment to commit to trying something new is an opportunity to learn and potentially find a new way of working the team likes better. And if it doesn't work, throw the idea in the bin and go back to what worked well or try something different again.

Retro is a space where psychological saftey is a must. When participating in retro, be open minded, honest, empathetic, and professional. There are ways to be direct and offer constructive feedback without being critical. Focus on the team and the impact/problem, rather than an individual's actions. There are so [many types of retrospectives](https://www.parabol.co/resources/sprint-retrospective-ideas/) out there, try one that best suits the team to get honest discussion going and generate some action items.

Another helpful tip is to assign someone to run the retro (timing, organization, action items and follow up). If you can’t find someone on your team, ask around. In order to remain objective, take notes, and monitor saftey, typically those that run retros don’t participate directly. It's not always possible to find someone external to your team to help lead, so having someone part of the team run the retro is perfectly acceptable. A good resource for running retros is the book [Agile Retrospectives](https://www.amazon.com/Agile-Retrospectives-Making-Teams-Great/dp/0977616649).

# In Summary

None of these suggestions are hard and fast rules. Rules are meant to be broken, so take the suggestions and try them out. Some will fit your team and your company culutre and others may not. It's all about learning and iterating, both on the team and on the product you're building. I hope these guidelines can help you and your team's efforts to deliver higher quality software with reduced effort.
